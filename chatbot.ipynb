{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Data Extraction and filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries\n",
    "import pandas as pd\n",
    "import re\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for filtering the whatsapp exported file \n",
    "def preprocess(file_path:str)->pd.DataFrame:\n",
    "    encryption_message=\"Messages and calls are end-to-end encrypted. Only people in this chat can read, listen to, or share them. Learn more.\\n\"\n",
    "    grppermission=r\"\\s*(.*) changed this group's settings to allow \\s*(.*)\"\n",
    "    media_pattern=\"<Media omitted>\"\n",
    "    email_pattern=r\"[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\"\n",
    "    url_pattern=r\"https?://[^\\s]+\"\n",
    "    edited_message=\"<This message was edited>\"\n",
    "    deleted_message=\"You deleted this message\"\n",
    "    grp_deleted=\"This message was deleted\"\n",
    "    null_message=\"null\"\n",
    "    created_group=\"created group\"\n",
    "    added_you=\"added you\"\n",
    "    tagging_pattern=r'@[\\w]+'\n",
    "    noti=r\"Your security code with \\s*(.*)\"\n",
    "    no_change=r\"\\s*(.*) changed their phone number to a new number. Tap to message or add the new number.\"\n",
    "    vcfmsg=r\".*?\\.vcf \\s*\\(file attached\\)\"\n",
    "    \n",
    "    with open(file_path,'r',encoding='utf-8') as f:\n",
    "        lines=f.readlines()\n",
    "    filtered_lines=[]\n",
    "    for line in lines:\n",
    "        if(encryption_message not in line and\n",
    "           deleted_message not in line and\n",
    "           grp_deleted not in line and\n",
    "           null_message != line.split(\" \")[-1] and\n",
    "           media_pattern not in line and\n",
    "           created_group not in line and\n",
    "           added_you not in line and\n",
    "           not re.search(email_pattern,line) and\n",
    "           not re.search(url_pattern,line) and \n",
    "           not re.search(noti,line) and \n",
    "           not re.search(no_change,line) and not re.search(vcfmsg,line) and not re.search(grppermission,line)):\n",
    "            line=line.replace(edited_message,\"\").strip()\n",
    "            line=re.sub(tagging_pattern,\"\",line).strip()\n",
    "            filtered_lines.append(line)\n",
    "    content='\\n'.join(filtered_lines)\n",
    "    content=content.replace('\\u202f',\" \")#clean invisible lines\n",
    "    # print(content)\n",
    "    \n",
    "    #regex for message \n",
    "    \n",
    "    pattern= r'(\\d{2}/\\d{2}/\\d{4}), (\\d{1,2}:\\d{2}\\s?[APMapm]{2}) - ([^:]+):\\s*(.*)'\n",
    "    pattern24= r'(\\d{2}/\\d{2}/\\d{4}), (\\d{2}:\\d{2}) - ([^:]+):\\s*(.*)'\n",
    "    messages=re.findall(pattern,content)\n",
    "    messages24=re.findall(pattern24,content)\n",
    "    # print(messages)\n",
    "    # print(messages24)\n",
    "    combined=[(f\"{d}, {t}\",s,m) for d,t,s,m in messages]\n",
    "    combined+=[(f\"{d}, {t}\",s,m) for d,t,s,m in messages24]\n",
    "    df=pd.DataFrame(combined,columns=['timestamp','sender','message'])\n",
    "    # df.insert(0,'id',range(0,len(df)))\n",
    "    # df.set_index('id', inplace=True)\n",
    "    #handling timestamp\n",
    "    \n",
    "    timestamps=[]\n",
    "    for timestamp in df['timestamp']:\n",
    "        try:\n",
    "            timestamp=pd.to_datetime(\n",
    "                timestamp,errors='coerce'\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(\"error\",e)\n",
    "            timestamp=pd.NaT\n",
    "        timestamps.append(timestamp)\n",
    "    df['timestamp']=timestamps\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_4992\\3372683218.py:59: UserWarning: Parsing dates in %d/%m/%Y, %H:%M format when dayfirst=False (the default) was specified. Pass `dayfirst=True` or specify a format to silence this warning.\n",
      "  timestamp=pd.to_datetime(\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_4992\\3372683218.py:59: UserWarning: Parsing dates in %d/%m/%Y, %H:%M am format when dayfirst=False (the default) was specified. Pass `dayfirst=True` or specify a format to silence this warning.\n",
      "  timestamp=pd.to_datetime(\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_4992\\3372683218.py:59: UserWarning: Parsing dates in %d/%m/%Y, %H:%M pm format when dayfirst=False (the default) was specified. Pass `dayfirst=True` or specify a format to silence this warning.\n",
      "  timestamp=pd.to_datetime(\n"
     ]
    }
   ],
   "source": [
    "file=preprocess(\"./dataset/WhatsApp Chat with Didi Jio.txt\")\n",
    "file2=preprocess(\"./dataset/WhatsApp Chat with Kartikey Dubey.txt\")\n",
    "file3=preprocess(\"./dataset/WhatsApp Chat with Akansha Shishodia.txt\")\n",
    "file4=preprocess(\"./dataset/WhatsApp Chat with I.T. 3rd year unofficial.txt\")\n",
    "file5=preprocess(\"./dataset/WhatsApp Chat with Hack it.txt\")\n",
    "file6=preprocess(\"./dataset/WhatsApp Chat with IT -III YEAR (LATERAL ENTRY) 2023-2026.txt\")\n",
    "file7=preprocess(\"./dataset/WhatsApp Chat with Mami.txt\")\n",
    "file8=preprocess(\"./dataset/WhatsApp Chat with IT Information.txt\")\n",
    "file9=preprocess(\"./dataset/WhatsApp Chat with Placement Information batch 2023.txt\")\n",
    "file10=preprocess(\"./dataset/whatsapp-chat-data.txt\")\n",
    "dataset=pd.concat([file,file2,file3,file4,file5,file6,file7,file8,file9,file10])\n",
    "dataset.to_csv(\"./dataset/chat.csv\",index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(38980, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_seq=(\" \").join(dataset['message'])\n",
    "with open('./output/text_seq.txt','w',encoding='utf-8') as f:\n",
    "    f.write(text_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "manual bpe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def preprocess_text(text_seq):\n",
    "#     text_seq.lower()\n",
    "#     texta_seq=re.sub(r'[^\\w\\s]','',text_seq)\n",
    "#     texts=text_seq.split()\n",
    "#     return texts\n",
    "# #intial vocab\n",
    "# def get_vocab(corpus):\n",
    "#     vocab = defaultdict(int) # defautdict is a data structure which help in avoiding key error if unique word enter then the intialize its freq to zero so we can easily increase it\n",
    "#     for word in corpus:\n",
    "#         chars = ' '.join(list(word)) + ' </w>'  # Add </w> to mark end of word\n",
    "#         vocab[chars] += 1\n",
    "#     return vocab\n",
    "# # vocab=get_vocab(texts)\n",
    "# # print(len(vocab))\n",
    "# #count freq of adjecent symbol pairs\n",
    "# def get_stats(vocab):\n",
    "#     pairs=defaultdict(int)\n",
    "#     for word,freq in vocab.items():\n",
    "#         symbols=word.split()\n",
    "#         for i in range(len(symbols)-1):\n",
    "#             pair=(symbols[i],symbols[i+1])\n",
    "#             pairs[pair]+=freq\n",
    "#     return max(pairs,key=pairs.get), max(pairs.values())\n",
    "# # pair=get_stats(vocab)\n",
    "# # print(pair)\n",
    "# #merge\n",
    "# def merge_vocab(pair, vocab):\n",
    "#     pattern = re.escape(' '.join(pair))\n",
    "#     replacement = ''.join(pair)\n",
    "#     new_vocab = {}\n",
    "#     for word in vocab:\n",
    "#         new_word = re.sub(pattern, replacement, word)\n",
    "#         new_vocab[new_word] = vocab[word]\n",
    "#     return new_vocab\n",
    "# # result=merge_vocab(pair[0],vocab)\n",
    "# # print(result)\n",
    "# def bpe(corpus, num_merges=10):\n",
    "#     vocab = get_vocab(corpus)\n",
    "#     print(\"Initial Vocabulary:\")\n",
    "#     for word in vocab:\n",
    "#         print(word)\n",
    "#     print(\"\\nStarting BPE Merges:\\n\")\n",
    "#     for i in range(num_merges):\n",
    "#         pair, freq = get_stats(vocab)\n",
    "#         if not pair:\n",
    "#             break\n",
    "#         print(f\"Merge #{i+1}: {pair} -> {''.join(pair)} (Frequency: {freq})\")\n",
    "#         vocab = merge_vocab(pair, vocab)\n",
    "#     return vocab\n",
    "# corpus=preprocess_text(text_seq)\n",
    "# result=bpe(corpus,num_merges=11)\n",
    "# result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1233\n",
      "392160\n"
     ]
    }
   ],
   "source": [
    "# Automate BPE\n",
    "import sys\n",
    "sys.path.append('./')\n",
    "from minbpe import BasicTokenizer\n",
    "\n",
    "vocab_size=1234\n",
    "tokenizer = BasicTokenizer()\n",
    "tokenizer.train(text_seq,vocab_size)\n",
    "vocab=tokenizer.vocab\n",
    "vocab\n",
    "tokenizer.encode('Shree ganesh')\n",
    "# isko todega phir token wala no assign karega\n",
    "tokenizer.decode([83, 104, 524, 103, 271, 364, 104])\n",
    "max_vocab_id = list(tokenizer.vocab.keys())[-1]\n",
    "print(max_vocab_id)\n",
    "tokenizer.special_tokens = {\n",
    "    \"<|startoftext|>\": max_vocab_id + 1,\n",
    "    \"<|separator|>\": max_vocab_id + 2,\n",
    "    \"<|endoftext|>\": max_vocab_id + 3,\n",
    "    \"<|unk|>\": max_vocab_id + 4,\n",
    "    \"<|padding|>\": max_vocab_id + 5,\n",
    "}\n",
    "\n",
    "# for fine tuning special tokens\n",
    "# output\n",
    "print(len(tokenizer.encode(text_seq)))\n",
    "tokenizer.save(file_prefix=\"./output/my_tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocab_size(tokenizer:BasicTokenizer):\n",
    "    vocab_size=len(tokenizer.vocab)\n",
    "    special_token=len(tokenizer.special_tokens)\n",
    "    return vocab_size+special_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1239"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_vocab_size(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.manual_seed(4893)\n",
    "# hyperparameters\n",
    "# Transformer model hyperparameters\n",
    "block_size = 256     # Maximum context length for each input sequence\n",
    "n_embd = 256        # Embedding size (dimension of each token's vector)\n",
    "n_head = 4         # Number of attention heads\n",
    "n_layer = 4          # Number of transformer layers\n",
    "dropout = 0.2        # Dropout rate for regularization\n",
    "vocab_size=get_vocab_size(tokenizer)\n",
    "device='cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#head \n",
    "from typing import Optional, Tuple\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as f\n",
    "\n",
    "class Head(nn.Module):#small brain\n",
    "    \"\"\" One Head of Self-Attention\"\"\"\n",
    "    def __init__(self,head_size:int):\n",
    "        super().__init__()# It connects code to PyTorch’s engine.\n",
    "        # A Query vector: what am I looking for?\n",
    "        # A Key vector: what do I offer?\n",
    "        # A Value vector: what info do I carry?\n",
    "        self.key=nn.Linear(n_embd,head_size,bias=False)\n",
    "        self.query=nn.Linear(n_embd,head_size,bias=False)\n",
    "        self.value=nn.Linear(n_embd,head_size,bias=False)\n",
    "        self.register_buffer('tril',torch.tril(torch.ones(block_size,block_size)))#“Only look at this word and the ones before it — not ahead.”\n",
    "        self.dropout=nn.Dropout(dropout)#randomly turning off a few brain cells while learning.\n",
    "        #To avoid overthinking or memorizing everything. It's a trick to help the model generalize better.\n",
    "        self.head_size=head_size\n",
    "    def forward(self,x:torch.Tensor)->torch.Tensor:\n",
    "        _,T,_=x.shape#[batch_size, T, n_embd]\n",
    "        k=self.key(x)#Transforms the input into a key vector for each word.\n",
    "        q=self.query(x)#Transforms the input into a query vector for each word.\n",
    "        weights=q@k.transpose(-2,-1)*k.shape[-1]**-0.5\n",
    "        #“How well does this word (query) match with every other word (key)?”\n",
    "        weights=weights.masked_fill(self.tril[:T,:T]==0,float('-inf'))#This hides the future.\n",
    "        weights=f.softmax(weights,dim=-1)#\"How much should I pay attention to each word?\"\n",
    "        weights=self.dropout(weights)#Drop some attention values to avoid overfitting (same as before).\n",
    "        v=self.value(x)#Now we get the actual information from each word (value vector).\n",
    "        out=weights@v#“Give me a mix of values, based on how much I should pay attention to each one.”\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multihead attention\n",
    "# The idea is to have multiple heads of attention, each focusing on different parts of the input.\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"Multiple heads of self attention in parallel\"\"\"\n",
    "    def __init__(self,num_heads:int,head_size:int):\n",
    "        super().__init__()\n",
    "        self.heads=nn.ModuleList([Head(head_size) for _ in  range(num_heads)]) #This creates a list of heads, each with its own set of parameters. \n",
    "        self.projection=nn.Linear(num_heads*head_size,n_embd) #This combines the outputs of all heads into a single vector.\n",
    "        self.dropout=nn.Dropout(dropout) #This randomly turns off some neurons to prevent overfitting.\n",
    "    def forward(self,x:torch.Tensor)->torch.Tensor:\n",
    "        out=torch.cat([h(x) for h in self.heads],dim=-1)    #This concatenates the outputs of all heads into a single tensor.\n",
    "        out=self.projection(out) #This combines the outputs of all heads into a single vector.\n",
    "        out=self.dropout(out) #This randomly turns off some neurons to prevent overfitting.\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block\n",
    "# A block is a self-attention layer followed by a feed-forward layer\n",
    "class FeedForward(nn.Module):\n",
    "    \"\"\"A simple linear layer followed by a non-linearity\"\"\"\n",
    "    def __init__(self,n_embd:int):\n",
    "        super().__init__()\n",
    "        self.net=nn.Sequential(\n",
    "            nn.Linear(n_embd,n_embd*4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_embd*4,n_embd),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "    def forward(self,x:torch.Tensor)->torch.Tensor:\n",
    "        return self.net(x)\n",
    "    \n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer Block: Self-Attention + Feed Forward\"\"\"\n",
    "    def __init__(self,n_embd:int,n_head:int):\n",
    "        super().__init__()\n",
    "        head_size=n_embd//n_head\n",
    "        self.self_attention=MultiHeadAttention(n_head,head_size)\n",
    "        self.feed_forward=FeedForward(n_embd)\n",
    "        self.layer_norm_1=nn.LayerNorm(n_embd)\n",
    "        self.layer_norm_2=nn.LayerNorm(n_embd)\n",
    "    def forward(self,x:torch.Tensor)->torch.Tensor:\n",
    "        x=self.layer_norm_1(x+self.self_attention(x))\n",
    "        x=self.layer_norm_2(x+self.feed_forward(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assembling the model\n",
    "class GPTLanguageModel(nn.Module):\n",
    "    \"\"\" The full GPT Language Model\"\"\"\n",
    "    def __init__(self):\n",
    "        # vocab_size: size of the vocabulary\n",
    "        # n_embd: size of the embedding vector\n",
    "        # n_layer: number of transformer blocks\n",
    "        # n_head: number of attention heads\n",
    "        # block_size: maximum context length for each input sequence\n",
    "        # This is the main class that combines all the components of the model. It initializes the token and position embeddings, the transformer blocks, and the final linear layer.\n",
    "        super().__init__()\n",
    "        self.token_embedding=nn.Embedding(vocab_size,n_embd)\n",
    "        self.position_embedding=nn.Embedding(block_size,n_embd)\n",
    "        self.blocks=nn.Sequential(*[Block(n_embd,n_head) for _ in range(n_layer)])\n",
    "        self.final_layer_norm=nn.LayerNorm(n_embd)\n",
    "        self.final_linear_layer=nn.Linear(n_embd,vocab_size)\n",
    "        self.apply(self._init_weights)\n",
    "        \n",
    "    def _init_weights(self,module):\n",
    "        if isinstance(module,nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight,mean=0.0,std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module,nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight,mean=0.0,std=0.02)\n",
    "            # torch.nn.init.zeros_(module.weight)\n",
    "    def forward(self,input_tokens:torch.Tensor,targets:Optional[torch.tensor]=None)->torch.Tensor:\n",
    "        \"\"\" forward pass of the model\n",
    "        input_tokens: Tensor of input token indices of shape (batch_size, sequence_length)\n",
    "        targets: target token ids (for training)\n",
    "        returns: tupules of (logits, loss) where logits of shape (batch_size, block_size, vocab_size) and loss is the optional cross-entropy loss\n",
    "        \"\"\"\n",
    "        B,T=input_tokens.shape\n",
    "        token_embeddings=self.token_embedding(input_tokens)\n",
    "        position_embeddings=self.position_embedding(torch.arange(T,device=device)) #This creates a tensor of position indices for the input sequence.\n",
    "        x=token_embeddings+position_embeddings #This adds the token and position embeddings together. shape (B,T,n_embd)\n",
    "        x=self.blocks(x) #This applies the transformer blocks to the input embeddings. shape (B,T,n_embd)\n",
    "        x=self.final_layer_norm(x) #This applies layer normalization to the output of the transformer blocks. shape (B,T,n_embd)\n",
    "        logits=self.final_linear_layer(x) #This applies a linear layer to the output of the transformer blocks. shape (B,T,vocab_size) \n",
    "        # logits=logits.view(B*T,-1) #This reshapes the output to (B*T,vocab_size) for loss calculation.\n",
    "        if targets is None:\n",
    "            loss=None\n",
    "        else:\n",
    "            B,T,V=logits.shape\n",
    "            logits=logits.view(B*T,V)\n",
    "            targets=targets.view(B*T)\n",
    "            loss=f.cross_entropy(logits,targets)\n",
    "        return logits,loss\n",
    "    def generate(self,input_tokens:torch.Tensor,max_new_tokens:int)->torch.Tensor:\n",
    "        # This function generates new tokens based on the input tokens. \n",
    "        # It uses the model to predict the next token in the sequence and appends it to the input tokens.\n",
    "        # This process is repeated for the specified number of new tokens.\n",
    "        # input_tokens: Tensor of input token indices of shape (batch_size, sequence_length)\n",
    "        # max_new_tokens: number of new tokens to generate\n",
    "        # returns: Tensor of generated token indices of shape (batch_size, sequence_length + max_new_tokens)\n",
    "        \n",
    "        for _ in range(max_new_tokens): \n",
    "            cropped_input=input_tokens[:,-block_size:] #This ensures that the input sequence does not exceed the maximum context length.\n",
    "            # This is the main part of the generation process. It uses the model to predict the next token in the sequence.\n",
    "            logits,_=self(cropped_input)\n",
    "            # logits=logits[:,-1,:] #This selects the last token in the sequence for prediction.\n",
    "            logits=logits[:,-1,:] #This selects the last token in the sequence for prediction.\n",
    "            probs=f.softmax(logits,dim=-1) #This applies the softmax function to the logits to get probabilities for each token in the vocabulary.\n",
    "            idx_next=torch.multinomial(probs,num_samples=1) #This samples the next token from the predicted probabilities.\n",
    "            input_tokens=torch.cat((input_tokens,idx_next),dim=1) #This appends the predicted token to the input sequence.\n",
    "        return input_tokens "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.857623\n"
     ]
    }
   ],
   "source": [
    "model=GPTLanguageModel()\n",
    "model.to(device)\n",
    "print(sum(p.numel() for p in model.parameters())/1e6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 256, 1239]) None\n"
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "seq_len=block_size=256\n",
    "x=torch.randint(0,vocab_size,(batch_size,seq_len),device=device)\n",
    "x=x.to(device)\n",
    "logits,loss=model(x)\n",
    "print(logits.shape,loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|--token_embedding: Embedding(317,184parameters)\n",
      "|--position_embedding: Embedding(65,536parameters)\n",
      "|--blocks: Sequential(3,155,968parameters)\n",
      "    |--0: Block(788,992parameters)\n",
      "        |--self_attention: MultiHeadAttention(262,400parameters)\n",
      "            |--heads: ModuleList(196,608parameters)\n",
      "                |--0: Head(49,152parameters)\n",
      "                    |--key: Linear(16,384parameters)\n",
      "                    |--query: Linear(16,384parameters)\n",
      "                    |--value: Linear(16,384parameters)\n",
      "                    |--dropout: Dropout(0parameters)\n",
      "                |--1: Head(49,152parameters)\n",
      "                    |--key: Linear(16,384parameters)\n",
      "                    |--query: Linear(16,384parameters)\n",
      "                    |--value: Linear(16,384parameters)\n",
      "                    |--dropout: Dropout(0parameters)\n",
      "                |--2: Head(49,152parameters)\n",
      "                    |--key: Linear(16,384parameters)\n",
      "                    |--query: Linear(16,384parameters)\n",
      "                    |--value: Linear(16,384parameters)\n",
      "                    |--dropout: Dropout(0parameters)\n",
      "                |--3: Head(49,152parameters)\n",
      "                    |--key: Linear(16,384parameters)\n",
      "                    |--query: Linear(16,384parameters)\n",
      "                    |--value: Linear(16,384parameters)\n",
      "                    |--dropout: Dropout(0parameters)\n",
      "            |--projection: Linear(65,792parameters)\n",
      "            |--dropout: Dropout(0parameters)\n",
      "        |--feed_forward: FeedForward(525,568parameters)\n",
      "            |--net: Sequential(525,568parameters)\n",
      "                |--0: Linear(263,168parameters)\n",
      "                |--1: ReLU(0parameters)\n",
      "                |--2: Linear(262,400parameters)\n",
      "                |--3: Dropout(0parameters)\n",
      "        |--layer_norm_1: LayerNorm(512parameters)\n",
      "        |--layer_norm_2: LayerNorm(512parameters)\n",
      "    |--1: Block(788,992parameters)\n",
      "        |--self_attention: MultiHeadAttention(262,400parameters)\n",
      "            |--heads: ModuleList(196,608parameters)\n",
      "                |--0: Head(49,152parameters)\n",
      "                    |--key: Linear(16,384parameters)\n",
      "                    |--query: Linear(16,384parameters)\n",
      "                    |--value: Linear(16,384parameters)\n",
      "                    |--dropout: Dropout(0parameters)\n",
      "                |--1: Head(49,152parameters)\n",
      "                    |--key: Linear(16,384parameters)\n",
      "                    |--query: Linear(16,384parameters)\n",
      "                    |--value: Linear(16,384parameters)\n",
      "                    |--dropout: Dropout(0parameters)\n",
      "                |--2: Head(49,152parameters)\n",
      "                    |--key: Linear(16,384parameters)\n",
      "                    |--query: Linear(16,384parameters)\n",
      "                    |--value: Linear(16,384parameters)\n",
      "                    |--dropout: Dropout(0parameters)\n",
      "                |--3: Head(49,152parameters)\n",
      "                    |--key: Linear(16,384parameters)\n",
      "                    |--query: Linear(16,384parameters)\n",
      "                    |--value: Linear(16,384parameters)\n",
      "                    |--dropout: Dropout(0parameters)\n",
      "            |--projection: Linear(65,792parameters)\n",
      "            |--dropout: Dropout(0parameters)\n",
      "        |--feed_forward: FeedForward(525,568parameters)\n",
      "            |--net: Sequential(525,568parameters)\n",
      "                |--0: Linear(263,168parameters)\n",
      "                |--1: ReLU(0parameters)\n",
      "                |--2: Linear(262,400parameters)\n",
      "                |--3: Dropout(0parameters)\n",
      "        |--layer_norm_1: LayerNorm(512parameters)\n",
      "        |--layer_norm_2: LayerNorm(512parameters)\n",
      "    |--2: Block(788,992parameters)\n",
      "        |--self_attention: MultiHeadAttention(262,400parameters)\n",
      "            |--heads: ModuleList(196,608parameters)\n",
      "                |--0: Head(49,152parameters)\n",
      "                    |--key: Linear(16,384parameters)\n",
      "                    |--query: Linear(16,384parameters)\n",
      "                    |--value: Linear(16,384parameters)\n",
      "                    |--dropout: Dropout(0parameters)\n",
      "                |--1: Head(49,152parameters)\n",
      "                    |--key: Linear(16,384parameters)\n",
      "                    |--query: Linear(16,384parameters)\n",
      "                    |--value: Linear(16,384parameters)\n",
      "                    |--dropout: Dropout(0parameters)\n",
      "                |--2: Head(49,152parameters)\n",
      "                    |--key: Linear(16,384parameters)\n",
      "                    |--query: Linear(16,384parameters)\n",
      "                    |--value: Linear(16,384parameters)\n",
      "                    |--dropout: Dropout(0parameters)\n",
      "                |--3: Head(49,152parameters)\n",
      "                    |--key: Linear(16,384parameters)\n",
      "                    |--query: Linear(16,384parameters)\n",
      "                    |--value: Linear(16,384parameters)\n",
      "                    |--dropout: Dropout(0parameters)\n",
      "            |--projection: Linear(65,792parameters)\n",
      "            |--dropout: Dropout(0parameters)\n",
      "        |--feed_forward: FeedForward(525,568parameters)\n",
      "            |--net: Sequential(525,568parameters)\n",
      "                |--0: Linear(263,168parameters)\n",
      "                |--1: ReLU(0parameters)\n",
      "                |--2: Linear(262,400parameters)\n",
      "                |--3: Dropout(0parameters)\n",
      "        |--layer_norm_1: LayerNorm(512parameters)\n",
      "        |--layer_norm_2: LayerNorm(512parameters)\n",
      "    |--3: Block(788,992parameters)\n",
      "        |--self_attention: MultiHeadAttention(262,400parameters)\n",
      "            |--heads: ModuleList(196,608parameters)\n",
      "                |--0: Head(49,152parameters)\n",
      "                    |--key: Linear(16,384parameters)\n",
      "                    |--query: Linear(16,384parameters)\n",
      "                    |--value: Linear(16,384parameters)\n",
      "                    |--dropout: Dropout(0parameters)\n",
      "                |--1: Head(49,152parameters)\n",
      "                    |--key: Linear(16,384parameters)\n",
      "                    |--query: Linear(16,384parameters)\n",
      "                    |--value: Linear(16,384parameters)\n",
      "                    |--dropout: Dropout(0parameters)\n",
      "                |--2: Head(49,152parameters)\n",
      "                    |--key: Linear(16,384parameters)\n",
      "                    |--query: Linear(16,384parameters)\n",
      "                    |--value: Linear(16,384parameters)\n",
      "                    |--dropout: Dropout(0parameters)\n",
      "                |--3: Head(49,152parameters)\n",
      "                    |--key: Linear(16,384parameters)\n",
      "                    |--query: Linear(16,384parameters)\n",
      "                    |--value: Linear(16,384parameters)\n",
      "                    |--dropout: Dropout(0parameters)\n",
      "            |--projection: Linear(65,792parameters)\n",
      "            |--dropout: Dropout(0parameters)\n",
      "        |--feed_forward: FeedForward(525,568parameters)\n",
      "            |--net: Sequential(525,568parameters)\n",
      "                |--0: Linear(263,168parameters)\n",
      "                |--1: ReLU(0parameters)\n",
      "                |--2: Linear(262,400parameters)\n",
      "                |--3: Dropout(0parameters)\n",
      "        |--layer_norm_1: LayerNorm(512parameters)\n",
      "        |--layer_norm_2: LayerNorm(512parameters)\n",
      "|--final_layer_norm: LayerNorm(512parameters)\n",
      "|--final_linear_layer: Linear(318,423parameters)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Layer Name</th>\n",
       "      <th>Type</th>\n",
       "      <th>Parameters</th>\n",
       "      <th>Trainable</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>token_embedding</td>\n",
       "      <td>Embedding</td>\n",
       "      <td>317184</td>\n",
       "      <td>317184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>position_embedding</td>\n",
       "      <td>Embedding</td>\n",
       "      <td>65536</td>\n",
       "      <td>65536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>blocks.0.self_attention.heads.0.key</td>\n",
       "      <td>Linear</td>\n",
       "      <td>16384</td>\n",
       "      <td>16384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>blocks.0.self_attention.heads.0.query</td>\n",
       "      <td>Linear</td>\n",
       "      <td>16384</td>\n",
       "      <td>16384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>blocks.0.self_attention.heads.0.value</td>\n",
       "      <td>Linear</td>\n",
       "      <td>16384</td>\n",
       "      <td>16384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>blocks.3.feed_forward.net.3</td>\n",
       "      <td>Dropout</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>blocks.3.layer_norm_1</td>\n",
       "      <td>LayerNorm</td>\n",
       "      <td>512</td>\n",
       "      <td>512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>blocks.3.layer_norm_2</td>\n",
       "      <td>LayerNorm</td>\n",
       "      <td>512</td>\n",
       "      <td>512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>final_layer_norm</td>\n",
       "      <td>LayerNorm</td>\n",
       "      <td>512</td>\n",
       "      <td>512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>final_linear_layer</td>\n",
       "      <td>Linear</td>\n",
       "      <td>318423</td>\n",
       "      <td>318423</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                               Layer Name       Type  Parameters  Trainable\n",
       "0                         token_embedding  Embedding      317184     317184\n",
       "1                      position_embedding  Embedding       65536      65536\n",
       "2     blocks.0.self_attention.heads.0.key     Linear       16384      16384\n",
       "3   blocks.0.self_attention.heads.0.query     Linear       16384      16384\n",
       "4   blocks.0.self_attention.heads.0.value     Linear       16384      16384\n",
       "..                                    ...        ...         ...        ...\n",
       "95            blocks.3.feed_forward.net.3    Dropout           0          0\n",
       "96                  blocks.3.layer_norm_1  LayerNorm         512        512\n",
       "97                  blocks.3.layer_norm_2  LayerNorm         512        512\n",
       "98                       final_layer_norm  LayerNorm         512        512\n",
       "99                     final_linear_layer     Linear      318423     318423\n",
       "\n",
       "[100 rows x 4 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def print_summary(model:torch.nn.Module, indent:str=''):\n",
    "    \"\"\"Prints a summary of the model architecture and the number of parameters in each layer.\"\"\"\n",
    "    for name, child in model.named_children():\n",
    "        params= sum(p.numel() for p in child.parameters())\n",
    "        print(f\"{indent}|--{name}: {child.__class__.__name__}({params:,}parameters)\")\n",
    "        print_summary(child, indent + '    ')\n",
    "print_summary(model)\n",
    "def get_model_stats(model:torch.nn.Module):\n",
    "    stats=[]\n",
    "    for name,module in model.named_modules():\n",
    "        if len(list(module.children()))==0:\n",
    "            params=sum(p.numel() for p in module.parameters())\n",
    "            stats.append({'Layer Name':name,'Type':module.__class__.__name__,'Parameters':params,'Trainable':sum(p.numel() for p in module.parameters() if p.requires_grad)})\n",
    "    return pd.DataFrame(stats)\n",
    "        \n",
    "stats_df=get_model_stats(model)\n",
    "stats_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
